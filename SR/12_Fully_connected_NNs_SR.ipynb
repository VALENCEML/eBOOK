{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7504e033",
      "metadata": {
        "id": "7504e033"
      },
      "source": [
        "# 12. Plitke neuronske mreže"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4df0a062-8788-4b4d-8a4f-a911996ecadf",
      "metadata": {
        "id": "4df0a062-8788-4b4d-8a4f-a911996ecadf"
      },
      "source": [
        "Branislav Gerazov, FEEIT, CMUS"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "eab54b77-e08c-4337-aa41-21db4cb84a71",
      "metadata": {
        "id": "eab54b77-e08c-4337-aa41-21db4cb84a71"
      },
      "source": [
        "Koristeći samo jedan neuron, model mašinskog učenja može rešavati samo linearne probleme. Drugim rečima, ako želimo da naučimo funkciju, ona može generisati samo linearnu funkciju - **linearna regresija**, a u slučaju klasifikacije neuron može razdvojiti dve klase samo linijom u 2D ili ravni u prostoru sa višim dimenzijama - **logistička regresija**.\n",
        "\n",
        "Dakle, **nelinearni problemi** koji se obično sreću u praksi ne mogu se rešiti sa jednim neuronом.\n",
        "Zbog toga se softverski neuroni gotovo uvek koriste u arhitekturama u kojima su više neurona međusobno povezani.\n",
        "Ove strukture nazivamo **neuronske mreže**."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7fc1c1ce-197c-40be-83c3-c8b8601f634d",
      "metadata": {
        "id": "7fc1c1ce-197c-40be-83c3-c8b8601f634d"
      },
      "source": [
        "## 12.0. Arhitektura plitkih neuronskih mreža\n",
        "\n",
        "U najjednostavnijem slučaju, neuronske mreže imaju jedan **skriveni sloj** i jedan **izlazni sloj** neurona, kao što je prikazano na slici.\n",
        "Takvi modeli se nazivaju **plitke neuronske mreže**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69be8255-29e1-4f49-b049-a88116b77964",
      "metadata": {
        "id": "69be8255-29e1-4f49-b049-a88116b77964"
      },
      "source": [
        "<img align=\"middle\" alt=\"Colored_neural_network\" src=\"https://github.com/VALENCEML/eBOOK/blob/main/EN/12/12_Colored_neural_network.png?raw=1\" width=\"300px\" style=\"display:block; margin-left: auto; margin-right: auto;\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3409121c-9896-476a-ab01-45216076c2fa",
      "metadata": {
        "id": "3409121c-9896-476a-ab01-45216076c2fa"
      },
      "source": [
        "**Slika 1.** Plitka neuronska mreža sa jednim skrivenim slojem koji se sastoji od četiri neurona (plavo) i jednim izlaznim slojem koji se sastoji od 2 neurona (zeleno). Ulazni podaci (karakteristike) ovde imaju 3 dimenzije i obično se prikazuju kao ulazni sloj (crveno).\n",
        "\n",
        "* [Wikimedia - Veštačka neuronska mreža](https://commons.wikimedia.org/w/index.php?curid=24913461)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c988bb83",
      "metadata": {
        "id": "c988bb83"
      },
      "source": [
        "Kao što se može videti na slici 1, svaki neuron skrivenog sloja je povezan sa svakim koeficijentom ulaznog vektora $\\mathbf{x}$.\n",
        "Svaki neuron u izlaznom sloju je zauzvrat povezan sa svakim neuronом u skrivenom sloju.\n",
        "\n",
        "Zbog ove međusobne povezanosti, ovaj tip slojeva se naziva **potpuno povezan** ili **gust** sloj.\n",
        "Iako jednostavna, ova arhitektura omogućava neuronskim mrežama da modeluju bilo koju nelinearnu funkciju, tj. da budu **univerzalni aproksimatori**.\n",
        "\n",
        "Izlaz plitke neuronske mreže za zadati ulazni vektor podataka $\\mathbf{x}$ može se izračunati pomoću:\n",
        "\n",
        "$$\n",
        "    \\mathbf{y}_h = f_h(\\mathbf{a}_h) = f_h(\\mathbf{W}_h \\mathbf{x}^T + \\mathbf{b}_h) \\, , \\\\\n",
        "    \\mathbf{y} = f_o(\\mathbf{a}_o) = f_o(\\mathbf{W}_o \\mathbf{y}_h + \\mathbf{b}_o)  \\\\\n",
        "    \\mathbf{y} = f_o(\\mathbf{W}_o \\cdot f_h(\\mathbf{W}_h \\mathbf{x}^T + \\mathbf{b}_h) + \\mathbf{b}_o) \\, ,\n",
        "$$\n",
        "\n",
        "gde $_h$ označava parametre i izlaze dobijene iz skrivenog sloja, a $_o$ označava one iz izlaznog sloja.\n",
        "Ovog puta, pošto možemo imati više neurona u svakom sloju, njihove težine su raspodeljene duž redova matrica težina $\\mathbf{W}$, a njihove pristrasnosti sadržane su u kolonama vektora $\\mathbf{b}$.\n",
        "\n",
        "Izlaz mreže dobija se obradom ulaznih podataka sloj po sloj, sve dok se ne dostigne izlazni sloj mreže.\n",
        "Ovaj proces se naziva **forward pass** ili **feed forward**.\n",
        "\n",
        "* Duboke neuronske mreže imaju više skrivenih slojeva, o njima raspravljamo u sledećem poglavlju.\n",
        "Ako se na ulaz dovede sekvencija od $N$ uzoraka ulaznih podataka $\\mathbf{x}_n$, dobijamo:\n",
        "\n",
        "$$\n",
        "    \\mathbf{Y}_h = f_h(\\mathbf{A}_h) = f_h(\\mathbf{W}_h \\mathbf{X}^T + \\mathbf{b}_h) \\, , \\\\\n",
        "    \\mathbf{Y} = f_o(\\mathbf{A}_o) = f_o(\\mathbf{W}_o \\mathbf{Y}_h + \\mathbf{b}_o) \\\\\n",
        "    \\mathbf{Y} = f_o(\\mathbf{W}_o \\cdot f_h(\\mathbf{W}_h \\mathbf{X}^T + \\mathbf{b}_h) + \\mathbf{b}_o) \\, .\n",
        "$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5d9858be",
      "metadata": {
        "id": "5d9858be"
      },
      "source": [
        "## 12.1. Aktivacione funkcije\n",
        "\n",
        "Da bismo koristili algoritam gradijentnog spusta (GD) za obučavanje neuronskih mreža, sve aktivacione funkcije (izlazne nelinearnosti) neurona u mreži, kao i funkcija gubitka, moraju biti diferencijabilne.\n",
        "U suprotnom ne bi bilo moguće izračunati gradijent za svaki od parametara svakog neurona u mreži.\n",
        "\n",
        "Zato se ne može koristiti diskontinualna izlazna funkcija, npr. presecanje aktivacije sa oštrim pragom:\n",
        "$$\n",
        "   y = f(a) = \\begin{cases}\n",
        "     1 & \\mbox{if } a > 0.5 \\\\\n",
        "     0 & \\mbox{otherwise}\n",
        "     \\end{cases} \\, .\n",
        "$$\n",
        "\n",
        "U tu svrhu se koriste nekoliko kontinualnih (diferencijabilnih) funkcija.\n",
        "Pogledaćemo zasebno aktivacione funkcije u skrivenim i izlaznim slojevima unutar neuronske mreže.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "eb5e51c0-d2c9-4c65-a19e-d0429bbb457f",
      "metadata": {
        "id": "eb5e51c0-d2c9-4c65-a19e-d0429bbb457f"
      },
      "source": [
        "### Aktivacione funkcije u skrivenim slojevima\n",
        "\n",
        "Uobičajeni izbori za nelinearnosti u skrivenom sloju su:\n",
        "\n",
        "   - **sigmoid** - sa izlazom u opsegu od 0 do 1\n",
        "\n",
        "$$\n",
        "\\sigma(a) = \\frac{1}{1+e^{-a}}\n",
        "$$\n",
        "\n",
        "   - **hiperbolički tangens** - sa izlazom u opsegu od -1 do 1, i\n",
        "   \n",
        "$$\n",
        "\\tanh(a) = \\frac{e^{2a} - 1}{e^{2a} + 1}\n",
        "$$\n",
        "\n",
        "   - **polu-talasni ispravljač**\n",
        "$$\n",
        "ReLU(a) = \\begin{cases}\n",
        "     a & \\mbox{if } a > 0 \\\\\n",
        "     0 & \\mbox{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Polu-talasni ispravljač ima nekoliko prednosti, uključujući: jednostavnije izračunavanje izlaza neurona, bolje širenje gradijenta tokom procesa obuke, kao i retkost aktivacije neurona - sa slučajnim inicijalizacijama težina, polovina neurona će imati izlaz 0.\n",
        "Zbog toga se često primenjuje u skrivenim slojevima neuronskih mreža.\n",
        "\n",
        "Još jedan motiv za korišćenje ove nelinearnosti je asimetrija u odnosu na $y$-osu, što je analogno načinu rada bioloških neurona.\n",
        "\n",
        "Prikažimo ove aktivacione funkcije u Pythonu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7e963a0",
      "metadata": {
        "id": "e7e963a0",
        "outputId": "e23ee414-fab7-417f-d701-899b93bb7a9f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "a = np.linspace(-3, 3, 100)\n",
        "y_sigmoid = 1 / (1 + np.exp(-a))\n",
        "y_tanh = (np.exp(2 * a) - 1) / (np.exp(2 * a) + 1)\n",
        "y_relu = a.copy() \n",
        "y_relu[a < 0] = 0 \n",
        "plt.plot(a, y_sigmoid, lw=2, alpha=0.7)\n",
        "plt.plot(a, y_tanh, lw=2, alpha=0.7)\n",
        "plt.plot(a, y_relu, lw=2, alpha=0.7)\n",
        "plt.axis([-3.1, 3.1, -2.1, 2.1])\n",
        "plt.grid(True)\n",
        "plt.legend([\"sigmoid\", \"tanh\", \"ReLU\"])\n",
        "plt.xlabel(\"Activation\")\n",
        "plt.ylabel(\"Neuron Output\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "41e05279-7e4e-4e79-a328-0e7691cd53ba",
      "metadata": {
        "id": "41e05279-7e4e-4e79-a328-0e7691cd53ba"
      },
      "source": [
        "### Aktivacione funkcije u izlaznom sloju\n",
        "\n",
        "Za neurone u izlaznom sloju, često se koriste:\n",
        "\n",
        "   - **sigmoid** - za klasifikaciju,\n",
        "   - **softmax** - za klasifikaciju sa više izlaznih klasa $J$:\n",
        "\n",
        "$$\n",
        "f(a_j) = \\frac{e^{a_j}}{\\sum_{j=0}^{J-1} e^{a_j}} \\, ,\n",
        "$$\n",
        "\n",
        "gde je $a_j$ aktivacija neurona koji odgovara klasi $j$; softmax funkcija normalizuje zbir izlaza svih izlaznih neurona, tako da se može smatrati aproksimacijom verovatnoće svake od klasa $f(a_j) \\approx P(y = j \\mid \\mathbf{a})$,\n",
        "\n",
        "   - **linearna** - u regresionim modelima\n",
        "\n",
        "$$\n",
        "f(a) = a\n",
        "$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8b5edb9d-952a-419e-8471-4a402ed75cd8",
      "metadata": {
        "id": "8b5edb9d-952a-419e-8471-4a402ed75cd8",
        "tags": []
      },
      "source": [
        "## 12.2. Funkcija gubitka\n",
        "\n",
        "Izbor nelinearnosti u izlaznom sloju odrediće izbor funkcije gubitka mreže.\n",
        "Neke funkcije gubitka imaju povoljnije karakteristike za neke izlazne nelinearnosti u odnosu na druge.\n",
        "\n",
        "Često se koriste sledeće funkcije grešaka:\n",
        "\n",
        "   - **srednje kvadratna greška** - osnovna funkcija greške za regresiju i binarnu klasifikaciju\n",
        "\n",
        "$$\n",
        "MSE = \\frac{1}{N} \\sum_{n=0}^{N-1} (y - \\tilde{y})^2\n",
        "$$\n",
        "\n",
        "   - **unakrsna entropija** - ima derivat sa boljim karakteristikama u klasifikacionim modelima u kojima je izlazna nelinearnost sigmoid,\n",
        "\n",
        "$$\n",
        "CE = - \\frac{1}{N} \\sum_{n=0}^{N-1} y \\ln \\tilde{y} + (1-y) \\ln(1-\\tilde{y})\n",
        "$$\n",
        "   - **log-likelihood** - u modelima sa softmax izlaznom funkcijom.\n",
        "\n",
        "$$\n",
        "LL = - \\ln \\tilde{y}\n",
        "$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e54cff9c-8ae4-4309-a465-6ea24fed14a5",
      "metadata": {
        "id": "e54cff9c-8ae4-4309-a465-6ea24fed14a5",
        "tags": []
      },
      "source": [
        "## 12.3. Obuka plitkih neuronskih mreža\n",
        "\n",
        "Kao što smo rekli u prethodnom poglavlju, neuronske mreže se obično obučavaju pomoću **algoritma gradijentnog spusta (GD ili Gradiend descent)**."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c8d77eb9-55c9-4eec-8f28-aaf6ccbd9e93",
      "metadata": {
        "id": "c8d77eb9-55c9-4eec-8f28-aaf6ccbd9e93"
      },
      "source": [
        "Kao što je spomenuto u poglavlju 11., u plitkim i dubokim neuronskim mrežama, proračun gradijenta za podešavanje parametara $\\theta$ počinje proračunom gradijenata za izlazni sloj, a zatim se vraća kroz (sve) skrivene slojeve.\n",
        "Ovom procesu se daje ime **povratno propagiranje** (backpropagation).\n",
        "\n",
        "* Proces povratnog propagiranja gradijenta je izraženiji u dubokim neuronskim mrežama koje imaju više skrivenih slojeva.\n",
        "\n",
        "**Lančano pravilo** iz matematičke analize se koristi za izračunavanje parcijalnih izvoda u odnosu na svaki parametar mreže:\n",
        "$$\n",
        "   \\frac{\\partial \\mathcal{L}}{\\partial \\theta_l} =\n",
        "   \\frac{\\partial \\mathcal{L}}{\\partial \\tilde{y}}\n",
        "   \\cdot \\frac{\\partial \\tilde{y}}{\\partial y_{L-2}}\n",
        "   \\, \\cdots \\, \\frac{\\partial y_{l+1}}{\\partial \\theta_{l+1}}\n",
        "   \\cdot \\frac{\\partial y_{l}}{\\partial \\theta_{l}}\n",
        "   \\, ,\n",
        "$$\n",
        "\n",
        "gde je $y_l$ izlaz $l$-tog sloja mreže i $L$ je ukupan broj slojeva."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "32a59d4c-9fe5-4276-afd5-329c454d31b0",
      "metadata": {
        "id": "32a59d4c-9fe5-4276-afd5-329c454d31b0"
      },
      "source": [
        "### Inicijalizacija parametara mreže\n",
        "\n",
        "Obično se početne vrednosti parametara mreže određuju nasumično tokom njene inicijalizacije.\n",
        "Trebali bismo shvatiti da će tačna vrednost parametara uticati na proces učenja.\n",
        "Tako, ako pretpostavimo da funkcija gubitka $\\mathcal{L}$ zavisi od parametra $\\theta$ kako je prikazano na slici 2, onda možemo videti da bi obuka mreže bila brža ako je početna vrednost $\\theta$ bliža minimumu $\\mathcal{L}$, na primer u poziciji $\\theta_B$ ili $\\theta_C$ umesto $\\theta_A$.\n",
        "Naravno, ne znamo gde se nalazi minimum $\\mathcal{L}$, ali može biti korisno obaviti nekoliko obuka na istoj mreži polazeći od različitih početnih vrednosti parametara."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a508849f-1d40-44c7-a835-76971d2d23cd",
      "metadata": {
        "id": "a508849f-1d40-44c7-a835-76971d2d23cd"
      },
      "source": [
        "<img align=\"middle\" alt=\"Gradient descend start positions\" src=\"https://github.com/VALENCEML/eBOOK/blob/main/EN/12/12_gradient_descend_start.png?raw=1\" width=\"600px\" style=\"display:block; margin-left: auto; margin-right: auto;\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5b24e749-9e9a-4124-bd5f-072665ffaadc",
      "metadata": {
        "id": "5b24e749-9e9a-4124-bd5f-072665ffaadc"
      },
      "source": [
        "Slika 2. Izbor početne vrednosti parametra $\\theta$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5f33370f-ddb4-451a-a139-fc5d341fc8e1",
      "metadata": {
        "id": "5f33370f-ddb4-451a-a139-fc5d341fc8e1"
      },
      "source": [
        "Problem izbora početne tačke je još veći ako funkcija gubitka $\\mathcal{L}$ ima složeniju zavisnost od parametra $\\theta$, kao na slici 3. Tada izbor početne vrednosti $\\theta$ može biti ključan za optimalno obučavanje mreže. Iz tri ponuđene početne vrednosti, samo iz $\\theta_C$ možemo doći do optimalne vrednosti $\\theta$ za koju $\\mathcal{L}$ ima najmanju vrednost, nazvanu globalni minimum, kroz gradijentni spust. U suprotnom, ako počnemo od $\\theta_B$ ili $\\theta_A$, proces obuke završiće se sa vrednostima $\\theta$ za koje vrednost $\\mathcal{L}$ ima **lokalne minimume**.\n",
        "\n",
        "* Imajte na umu da je $\\theta_B$ bliže globalnom minimumu od $\\theta_C$!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9459f25-e2ed-43b6-ab3c-b719452d04bf",
      "metadata": {
        "id": "c9459f25-e2ed-43b6-ab3c-b719452d04bf"
      },
      "source": [
        "<img align=\"middle\" alt=\"Gradient descend global and local minimum\" src=\"https://github.com/VALENCEML/eBOOK/blob/main/EN/12/12_gradient_descend_start_global_local.png?raw=1\" width=\"600px\" style=\"display:block; margin-left: auto; margin-right: auto;\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5200fa0f-41f4-4089-8f47-ca85b8a63ec0",
      "metadata": {
        "id": "5200fa0f-41f4-4089-8f47-ca85b8a63ec0"
      },
      "source": [
        "**Slika 3.** Primer početnih vrednosti parametra $\\theta$ sa složenijom funkcijom gubitka."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0c8108bc-fdbd-4938-a3d9-f7251dfef286",
      "metadata": {
        "id": "0c8108bc-fdbd-4938-a3d9-f7251dfef286"
      },
      "source": [
        "### Stopa učenja\n",
        "\n",
        "Kada koristimo GD, stopa učenja je jedan od najvažnijih parametara u obuci neuronskih mreža. Ako je previše velika, optimizacija može promašiti minimum funkcije greške, dok ako je previše mala, algoritmu će biti potrebno mnogo iteracija da završi obuku."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae63ba4a-2db2-4638-a3a8-58b344957afb",
      "metadata": {
        "id": "ae63ba4a-2db2-4638-a3a8-58b344957afb"
      },
      "source": [
        "<img align=\"middle\" alt=\"Gradient descend large learning rate\" src=\"https://github.com/VALENCEML/eBOOK/blob/main/EN/12/12_gradient_descend_large_rate.png?raw=1\" width=\"600px\" style=\"display:block; margin-left: auto; margin-right: auto;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e05e7a2a-2de6-42e4-895f-e8ccb631b3c8",
      "metadata": {
        "id": "e05e7a2a-2de6-42e4-895f-e8ccb631b3c8"
      },
      "source": [
        "<img align=\"middle\" alt=\"Gradient descend small learning rate\"  src=\"https://github.com/VALENCEML/eBOOK/blob/main/EN/12/12_gradient_descend_small_rate.png?raw=1\" width=\"600px\" style=\"display:block; margin-left: auto; margin-right: auto;\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "03dedab0-8f57-4c63-a6c3-d41c1ae1a716",
      "metadata": {
        "id": "03dedab0-8f57-4c63-a6c3-d41c1ae1a716"
      },
      "source": [
        "Slika 4. Ilustracija uticaja koraka učenja na proces obuke: kada je korak velik, mreža će brzo ići prema minimumu funkcije gubitka, ali neće moći da ga dostigne; kada je korak mali, mreža može dostići minimum, ali obuka će zahtevati veći broj iteracija."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e86df5ca-b86c-4a2b-8c21-db421dfe03cb",
      "metadata": {
        "id": "e86df5ca-b86c-4a2b-8c21-db421dfe03cb"
      },
      "source": [
        "Kako bi se optimizovao proces učenja, obično se koristi **planiranje stope učenja**.\n",
        "\n",
        "Jedna jednostavna strategija je da se koristi veća stopa učenja na početku obuke, kada mreža \"ne zna\" ništa o problemu, kako bi se postigao brži napredak.\n",
        "Zatim se ovaj prvobitno veliki korak postepeno smanjuje kako bi obuka mogla da konvergira bliže minimumu funkcije gubitka.\n",
        "\n",
        "Postoje i napredniji algoritmi, kao što je **Adam**, koji se često koristi u obuci neuronskih mreža. On uzima u obzir prve i druge momente, tj. brzinu i ubrzanje promene gradijenta u odnosu na prethodne iteracije za prilagođavanje koraka učenja.\n",
        "\n",
        "Jedan od glavnih problema u obuci neuronskih mreža je mogućnost da se proces zaglavi u lokalnom minimumu, uz trošak propuštanja globalnog minimuma.\n",
        "Ovo je izraženije u manjim neuronskim mrežama.\n",
        "Ovaj problem može biti rešen reinicijalizacijom koraka učenja na početnu veliku vrednost nakon određenog broja iteracija."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a1a935d2-8bcd-4623-ad1d-09d883a0f26c",
      "metadata": {
        "id": "a1a935d2-8bcd-4623-ad1d-09d883a0f26c"
      },
      "source": [
        "### Podela skupa za obuku na serije\n",
        "\n",
        "Kod obuke neuronskih mreža, skupovi podataka su obično veliki i ne mogu se čuvati u radnoj memoriji, pa je potrebno podeliti podatke na delove tokom obuke.\n",
        "\n",
        "Jedan ekstrem je optimizacija parametara mreže sa gradijentom izračunatim za svaki od uzoraka iz skupa za obuku pojedinačno.\n",
        "Ova varijanta GD algoritma se naziva stohastički gradijentni spust.\n",
        "Prolazak kroz čitav skup za obuku naziva se epoha.\n",
        "Randomizacija ulaznih uzoraka se vrši nasumičnim mešanjem skupa podataka pre svake epohe, a zatim sekvencijalnim uzorkovanjem.\n",
        "\n",
        "Međutim, izračunavanje gradijenta po uzorku ne pruža dobru procenu stvarnog gradijenta funkcije gubitka.\n",
        "Rešenje je upotreba podskupa ili serije (batch) uzoraka iz skupa za obuku uzetih nasumično, kako bi se izračunao gradijent i ažurirali parametri.\n",
        "Ova verzija GD algoritma se naziva **gradijentni spust serije (BGD)* i gradijentni spust mini-serije (MBGD)**.\n",
        "\n",
        "* Tehnički postoji razlika između MBGD-a i BGD-a - kod MBGD-a se parametri ažuriraju za svaku seriju, dok se kod BGD-a to radi nakon prolaska kroz ceo skup za obuku.\n",
        "\n",
        "Skoro uvek za obuku neuronskih mreža i drugih algoritama mašinskog učenja koristi se MBGD algoritam, ali pod imenom SGD."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c24aa151",
      "metadata": {
        "id": "c24aa151"
      },
      "source": [
        "## 12.4. Regularizacija u neuronskim mrežama\n",
        "\n",
        "Kod obučavanja neuronskih mreža postoji opasnost od **preprilagođavanja** parametara neuronske mreže. To se događa kada mreža postane preterano prilagođena trening skupu, i kao rezultat toga, njene performanse na test skupu se pogoršavaju. Stoga je potrebno kažnjavati preprilagođavanje tokom procesa obučavanja. Ovo se naziva **regularizacija**.\n",
        "\n",
        "S obzirom da se kod preprilagođavanja parametri mreže dobijaju izuzetno visoke vrednosti, jedan način za regularizaciju je da se uvede član u funkciji gubitka koji će ih kažnjavati. To se postiže uvođenjem $L^2$ norme $\\theta$ modela parametara. Na primer, koristeći srednju kvadratnu grešku kao primer, imali bismo:\n",
        "\n",
        "$$\n",
        "   L(y, \\tilde{y}, \\theta) = L(y, \\tilde{y}) + \\lambda \\sum_{l=0}^{L-1} (\\mathbf{W}_l^T \\mathbf{W}_l + \\mathbf{b}_l^T \\mathbf{b}_l) \\, ,\n",
        "$$\n",
        "\n",
        "gde je $\\lambda$ **koeficijent regularizacije**, a $\\mathbf{W}_l^T \\mathbf{W}_l$ i $\\mathbf{b}_l^T \\mathbf{b}_l$ daju zbir svih kvadriranih parametara neurona sloja $l$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b5273e9d-1b1c-450a-95db-725680f2953b",
      "metadata": {
        "id": "b5273e9d-1b1c-450a-95db-725680f2953b"
      },
      "source": [
        "## Regresija sa plitkom neuronskom mrežom\n",
        "\n",
        "Napravimo neuronsku mrežu kako bismo predvideli izlaz sinusne funkcije za ulazne podatke koji nisu deo trening skupa. Da bismo radili sa neuronskim mrežama, koristićemo paket `scikit-learn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1666cb4-7f46-4935-8fd9-b82a7a7371a1",
      "metadata": {
        "id": "a1666cb4-7f46-4935-8fd9-b82a7a7371a1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import neural_network"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "610cea62-1d13-48fc-91f9-c9dde7302da6",
      "metadata": {
        "id": "610cea62-1d13-48fc-91f9-c9dde7302da6"
      },
      "source": [
        "Prvo, napravimo trening skup. Uzećemo 10 jednako razmaknutih vrednosti u intervalu od 0 do 2$\\pi$ i izračunati sinusne vrednosti za njih. Pored sinusne funkcije, dodajemo i beli šum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d57d0bec-61ce-4661-961b-9b7a31a61179",
      "metadata": {
        "id": "d57d0bec-61ce-4661-961b-9b7a31a61179"
      },
      "outputs": [],
      "source": [
        "xs = np.linspace(0, 2 * np.pi, 10)\n",
        "np.random.seed(42)\n",
        "ys = np.sin(xs) + np.random.normal(size=xs.size) * 0.2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a18f028d-bc60-4924-b46d-6fe4d9483d9d",
      "metadata": {
        "id": "a18f028d-bc60-4924-b46d-6fe4d9483d9d"
      },
      "source": [
        "Prikažimo dobijeni trening skup i sinusnu funkciju koja će predstavljati ciljnu funkciju koju želimo naučiti."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "946e7f8a-998a-4496-90ba-a202f99cb64f",
      "metadata": {
        "id": "946e7f8a-998a-4496-90ba-a202f99cb64f",
        "outputId": "8cbea999-0c3b-4a77-97a1-a83bf2ed0a4e"
      },
      "outputs": [],
      "source": [
        "x_axis = np.linspace(-.1, 2 * np.pi + .1, 100)\n",
        "y_sin = np.sin(x_axis)\n",
        "\n",
        "xs = np.expand_dims(xs, 1)\n",
        "x_axis = np.expand_dims(x_axis, 1)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(xs, ys)\n",
        "plt.plot(x_axis, y_sin, ':', lw=4, alpha=0.2)\n",
        "plt.grid()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7925eacc-865e-426d-8c52-1dd6495bac87",
      "metadata": {
        "id": "7925eacc-865e-426d-8c52-1dd6495bac87"
      },
      "source": [
        "Inicijalizujmo neuronsku mrežu klasom `MLPRegressor`, koja će kreirati neuronsku mrežu sa linearnom izlaznom funkcijom u neuronu izlaznog sloja."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee06725b-3c2c-45e2-9a9e-5bfe18dcd185",
      "metadata": {
        "id": "ee06725b-3c2c-45e2-9a9e-5bfe18dcd185"
      },
      "outputs": [],
      "source": [
        "reg = neural_network.MLPRegressor(\n",
        "    hidden_layer_sizes=(5),\n",
        "    activation=\"tanh\",\n",
        "    solver=\"adam\",\n",
        "    alpha=0.0001,\n",
        "    batch_size=\"auto\",\n",
        "    learning_rate_init=0.01,\n",
        "    max_iter=50,\n",
        "    tol=1e-4,\n",
        "    early_stopping=False,\n",
        "    random_state=42,\n",
        "    verbose=True,    \n",
        "    )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f1d19451-4fc0-4adb-b5e6-4f9999773f49",
      "metadata": {
        "id": "f1d19451-4fc0-4adb-b5e6-4f9999773f49"
      },
      "source": [
        "Sa ulaznim parametrima definišemo sledeće vrednosti:\n",
        "- `hidden_layer_sizes` - broj neurona u skrivenom sloju mreže,\n",
        "- `activation` -  izbor izlazne nelinearnosti neurona u skrivenom sloju mreže,\n",
        "- `solver` -  izbor algoritma za učenje,\n",
        "- `alpha` - koeficijent $L^2$ regularizacije, koji smo označili gore sa $\\lambda$,\n",
        "- `batch_size` -  veličina serije (batch-a),\n",
        "- `learning_rate_init` - početna vrednost koraka učenja,\n",
        "- `max_iter` - maksimalni broj epoha obučavanja mreže,\n",
        "- `tol` - ako se gubitak promeni za manje od ovog praga tolerancije tokom 10 epoha, obučavanje će biti zaustavljeno uprkos tome što maksimalni broj epoha nije dostignut,\n",
        "- `early_stopping` -  parametar koji omogućava rano zaustavljanje obučavanja ako gubici izračunati na validacionom podskupu počnu rasti, koji se automatski odvaja od trening skupa nasumično,\n",
        "- `random_state` -  podešavanje generatora slučajnih brojeva kako bismo dobili iste parametre mreže tokom njene slučajne inicijalizacije,\n",
        "- `verbose` -  nivo detalja izlaza u procesu obučavanja mreže.\n",
        "\n",
        "Svi ovi argumenti koji opisuju arhitekturu mreže i koji kontrolišu proces obučavanja poznati su kao hiperparametri, kako bi ih razlikovali od stvarnih parametara neurona u mreži - pri čemu se ovi poslednji ažuriraju u procesu obučavanja, a ovi prvi ne.\n",
        "\n",
        "Da bismo videli sve dostupne postavke za neuronsku mrežu, kao i više detalja o svakoj od njih, možemo otkucati `neural_network.MLPRegressor?`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c98eb29b-1ab0-4670-8831-d6f2dde1bdd5",
      "metadata": {
        "id": "c98eb29b-1ab0-4670-8831-d6f2dde1bdd5"
      },
      "source": [
        "Nakon što smo inicijalizovali mrežu, možemo je obučiti sledećom komandom:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0256c0fe-2547-41dc-b6a7-bc79dcb1c6bf",
      "metadata": {
        "id": "0256c0fe-2547-41dc-b6a7-bc79dcb1c6bf",
        "outputId": "b7b4b6c6-fde0-4a38-c020-5f1d66b3e8e6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "reg.fit(xs, ys)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f50a7c30-fb26-4ebe-a89a-15ed538abbc0",
      "metadata": {
        "id": "f50a7c30-fb26-4ebe-a89a-15ed538abbc0"
      },
      "source": [
        "Iz ispisa možemo videti da obučavanje nije konvergiralo, odnosno, gubitak je i dalje opadao pre nego što je obučavanje zaustavljeno zbog dostizanja maksimalnog broja epoha. Drugim rečima, naša mreža se **nedovoljno prilagođava (under fitting)**.\n",
        "\n",
        "Sada možemo koristiti obučenu mrežu za predviđanje vrednosti naučene funkcije:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a6e73ff-b87c-4833-9df2-a4eb860aa285",
      "metadata": {
        "id": "6a6e73ff-b87c-4833-9df2-a4eb860aa285"
      },
      "outputs": [],
      "source": [
        "y_pred_under = reg.predict(x_axis)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1097335a-aa24-4bcc-8755-fed4885a701e",
      "metadata": {
        "id": "1097335a-aa24-4bcc-8755-fed4885a701e"
      },
      "source": [
        "I prikazati rezultate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "798951c4-5a5f-439c-9f85-e9c3bf948ea6",
      "metadata": {
        "id": "798951c4-5a5f-439c-9f85-e9c3bf948ea6",
        "outputId": "cb6a49ed-f5ea-4e23-df65-4e69e92a924d",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# %% plot results\n",
        "plt.figure()\n",
        "plt.scatter(xs, ys, c='k')\n",
        "plt.plot(x_axis, y_sin, ':', lw=4, alpha=0.2)\n",
        "plt.plot(x_axis, y_pred_under)\n",
        "plt.grid()\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f780a486-b988-4f8e-a947-0175f3321d7d",
      "metadata": {
        "id": "f780a486-b988-4f8e-a947-0175f3321d7d"
      },
      "source": [
        "Vidimo da je mreža relativno dobro opisala tačke trening seta, ali nije naučila ciljnu funkciju.\n",
        "\n",
        "Da bismo ponovno obučili mrežu, inicijalizovaćemo je s novim parametrima i ponoviti obučavanje."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "157b55b0-89f1-430a-81af-cc9000d00a39",
      "metadata": {
        "id": "157b55b0-89f1-430a-81af-cc9000d00a39",
        "outputId": "0da33b3a-65e7-4e6c-acbe-15731f7795d7"
      },
      "outputs": [],
      "source": [
        "reg = neural_network.MLPRegressor(\n",
        "    hidden_layer_sizes=(5),\n",
        "    activation=\"tanh\",\n",
        "    alpha=0.0001,\n",
        "    learning_rate_init=0.01,\n",
        "    max_iter=500,\n",
        "    tol=1e-4,\n",
        "    random_state=42,\n",
        "    )\n",
        "reg.fit(xs, ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc879b88-2651-4e65-a229-74fbfd7fa2d1",
      "metadata": {
        "id": "fc879b88-2651-4e65-a229-74fbfd7fa2d1"
      },
      "source": [
        "Vidimo da je sada obučavanje konvergiralo. Prikazaćemo rezultate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1426ffd7-87e3-4421-b5d9-1a9d3e600c65",
      "metadata": {
        "id": "1426ffd7-87e3-4421-b5d9-1a9d3e600c65",
        "outputId": "2149981a-21be-4401-e451-5092d9d7b4d3",
        "tags": []
      },
      "outputs": [],
      "source": [
        "y_pred = reg.predict(x_axis)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(xs, ys, c='k')\n",
        "plt.plot(x_axis, y_sin, ':', lw=4, alpha=0.2)\n",
        "plt.plot(x_axis, y_pred_under, label=\"under fitting\")\n",
        "plt.plot(x_axis, y_pred, label=\"converged\")\n",
        "plt.grid()\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7c8a2f0-73a0-48b4-8ccf-cedf1426d6fa",
      "metadata": {
        "id": "d7c8a2f0-73a0-48b4-8ccf-cedf1426d6fa"
      },
      "source": [
        "Vidimo da je mreža nakon konvergiranja došla bliže ciljnoj funkciji.\n",
        "\n",
        "Prikažimo parametre obučene mreže:\n",
        "\n",
        "- poslednju vrednost gubitka,\n",
        "- broj epoha u obučavanju,\n",
        "- težine neurona u mreži, i\n",
        "- pristrasnosti neurona u mreži."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f46befcd-bea7-451c-8578-606028decf46",
      "metadata": {
        "id": "f46befcd-bea7-451c-8578-606028decf46",
        "outputId": "9c8f7984-0e25-4d7a-94f6-34a10304e421"
      },
      "outputs": [],
      "source": [
        "print(\"loss\", reg.loss_)\n",
        "print(\"epochs\", reg.n_iter_)\n",
        "print(\"weights\", reg.coefs_)\n",
        "print(\"biases\", reg.intercepts_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fee3930c-7988-4ca8-b941-14c66af23d39",
      "metadata": {
        "id": "fee3930c-7988-4ca8-b941-14c66af23d39"
      },
      "source": [
        "Vidimo da je obučavanje završeno u 274 epohe, a konačna vrednost gubitka iznosi 0,059. To je manje od poslednjeg gubitka kada smo zaustavili obučavanje u epohi 50 (0,14).\n",
        "\n",
        "Prikazaćemo i promenu gubitka tokom obučavanja."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11f5ffb4-fcb5-4ab8-88db-4204c3048a08",
      "metadata": {
        "id": "11f5ffb4-fcb5-4ab8-88db-4204c3048a08",
        "outputId": "27fa69f2-6ea5-403d-8947-7ddd86852c8e"
      },
      "outputs": [],
      "source": [
        "plt.plot(reg.loss_curve_)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef009d60-e8f3-41a9-b047-922f554ec847",
      "metadata": {
        "id": "ef009d60-e8f3-41a9-b047-922f554ec847"
      },
      "source": [
        "Već smo videli kako je promena jednog od hiperparametara (maksimalni broj epoha) uticala na proces obučavanja.\n",
        "\n",
        "Uradićemo neke eksperimente kako bismo videli kako ostali hiperparametri utiču na performanse dobijene mreže."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d9a7e69-96b8-4d2e-9e18-a4b2318125bf",
      "metadata": {
        "id": "5d9a7e69-96b8-4d2e-9e18-a4b2318125bf"
      },
      "source": [
        "### Eksperiment 1. Menjanje broja neurona"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8357ede",
      "metadata": {
        "id": "e8357ede",
        "outputId": "5af8b655-5491-4535-ca34-7be5ff26fdd0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.scatter(xs, ys)\n",
        "plt.plot(x_axis, y_sin, ':', lw=4, alpha=0.2)\n",
        "plt.grid()\n",
        "for neurons in [1, 2, 5]:\n",
        "    reg = neural_network.MLPRegressor(\n",
        "        hidden_layer_sizes=neurons,\n",
        "        activation=\"tanh\",\n",
        "        alpha=0.0001,\n",
        "        learning_rate_init=0.01,\n",
        "        max_iter=500,\n",
        "        tol=1e-4,\n",
        "        random_state=42,\n",
        "        )\n",
        "    reg.fit(xs, ys)\n",
        "    y_preds = reg.predict(x_axis)\n",
        "    plt.plot(x_axis, y_preds, lw=2, alpha=0.5, label=neurons)\n",
        "\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cea7c34-6b6f-4fbf-abf3-ee45b099b23d",
      "metadata": {
        "id": "0cea7c34-6b6f-4fbf-abf3-ee45b099b23d"
      },
      "source": [
        "Vidimo da s 1 neuronom u skrivenom sloju na izlazu mreže dobijamo njen izlazni nelinearni funkciju `tanh`. Već sa 2 neurona mreža može naučiti funkciju blisku ciljnoj. Ovo ilustruje potrebu za snagom modela da bude adekvatna problemu koji želimo da rešimo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "286645c4-ec00-40d5-928b-e180118c6272",
      "metadata": {
        "id": "286645c4-ec00-40d5-928b-e180118c6272"
      },
      "source": [
        "### Eksperiment 2. Menjanje stope učenja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24dce286-7c3b-4cfb-a435-d3645b9a9efe",
      "metadata": {
        "id": "24dce286-7c3b-4cfb-a435-d3645b9a9efe",
        "outputId": "f86db535-4e78-4a13-a220-250a63819604"
      },
      "outputs": [],
      "source": [
        "# %% try different learning rates\n",
        "plt.figure()\n",
        "plt.scatter(xs, ys)\n",
        "plt.plot(x_axis, y_sin, ':', lw=4, alpha=0.2)\n",
        "plt.grid()\n",
        "for learn in [0.001, 0.01, 0.1, 1]:\n",
        "    reg = neural_network.MLPRegressor(\n",
        "        hidden_layer_sizes=(5),\n",
        "        activation=\"tanh\",\n",
        "        alpha=0.0001,\n",
        "        learning_rate_init=learn,\n",
        "        max_iter=500,\n",
        "        tol=1e-4,\n",
        "        random_state=42,\n",
        "        )\n",
        "    reg.fit(xs, ys)\n",
        "    y_preds = reg.predict(x_axis)\n",
        "    plt.plot(x_axis, y_preds, lw=2, alpha=0.5, label=learn)\n",
        "\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "784be240-30a4-4626-bb95-aa99a04a9a5a",
      "metadata": {
        "id": "784be240-30a4-4626-bb95-aa99a04a9a5a"
      },
      "source": [
        "Grafikon jasno pokazuje uticaj stope učenja na proces obučavanja. Za vrlo velike vrednosti (1) možemo videti da je obučavanje potpuno propalo. Bolje rezultate dobijamo za vrednosti 0,1 i 0,01. Za male vrednosti (0,001) možemo videti da obučavanje nije konvergiralo unutar istog broja epoha kao za veće vrednosti."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba7c946b-b07d-434d-8971-fb70001b91c4",
      "metadata": {
        "id": "ba7c946b-b07d-434d-8971-fb70001b91c4",
        "tags": []
      },
      "source": [
        "### Eksperiment 3. Menjanje inicijalizacije obučavanja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95030c76",
      "metadata": {
        "id": "95030c76",
        "outputId": "3e391fdb-8f6d-4581-efeb-b31f61f530ee"
      },
      "outputs": [],
      "source": [
        "# %% try different starting points\n",
        "plt.figure()\n",
        "plt.scatter(xs, ys)\n",
        "plt.plot(x_axis, y_sin, ':', lw=4, alpha=0.2)\n",
        "plt.grid()\n",
        "for seed in [1, 10, 42, 100]:\n",
        "    reg = neural_network.MLPRegressor(\n",
        "        hidden_layer_sizes=(5),\n",
        "        activation=\"tanh\",\n",
        "        alpha=0.0001,\n",
        "        learning_rate_init=0.01,\n",
        "        max_iter=500,\n",
        "        tol=1e-4,\n",
        "        random_state=seed,\n",
        "        )\n",
        "    reg.fit(xs, ys)\n",
        "    y_preds = reg.predict(x_axis)\n",
        "    plt.plot(x_axis, y_preds, lw=2, alpha=0.5, label=seed)\n",
        "\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7b67a17-a82a-4d12-accf-71564e03d95f",
      "metadata": {
        "id": "f7b67a17-a82a-4d12-accf-71564e03d95f"
      },
      "source": [
        "Vidimo da zaista početna tačka utiče na konačni rezultat procesa obučavanja."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f2526c6-c4ee-4b89-b68f-ce34b1001b16",
      "metadata": {
        "id": "1f2526c6-c4ee-4b89-b68f-ce34b1001b16"
      },
      "source": [
        "### Eksperiment 4. Regularizacija"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a5235ff-f4be-48f3-82ce-2e2d89f85e8c",
      "metadata": {
        "id": "9a5235ff-f4be-48f3-82ce-2e2d89f85e8c"
      },
      "source": [
        "Da bismo ilustrovali potrebu za regularizacijom mreže, hajde da vidimo šta se događa ako smanjimo toleranciju zaustavljanja i povećamo maksimalni broj epoha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d19e4c7-cbbf-4fdc-b110-333d32e13058",
      "metadata": {
        "id": "5d19e4c7-cbbf-4fdc-b110-333d32e13058",
        "outputId": "9a34fc3f-a03b-4d96-d710-dbf5ad3e967d"
      },
      "outputs": [],
      "source": [
        "reg = neural_network.MLPRegressor(\n",
        "    hidden_layer_sizes=(5),\n",
        "    activation=\"tanh\",\n",
        "    alpha=0.0001,\n",
        "    learning_rate_init=0.01,\n",
        "    max_iter=5000,\n",
        "    tol=1e-9,\n",
        "    random_state=42,\n",
        "    )\n",
        "reg.fit(xs, ys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1deb9af2-9877-4774-9173-0a78578d882d",
      "metadata": {
        "id": "1deb9af2-9877-4774-9173-0a78578d882d",
        "outputId": "1766b29c-d413-4ca7-90ee-fb521f388df9",
        "tags": []
      },
      "outputs": [],
      "source": [
        "y_pred_over = reg.predict(x_axis)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(xs, ys, c='k')\n",
        "plt.plot(x_axis, y_sin, ':', lw=4, alpha=0.2)\n",
        "plt.plot(x_axis, y_pred_under, label=\"underfitting\")\n",
        "plt.plot(x_axis, y_pred, label=\"converged\")\n",
        "plt.plot(x_axis, y_pred_over, label=\"overfitting\")\n",
        "plt.grid()\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b314101-ab45-4dea-8336-3969a00f6204",
      "metadata": {
        "id": "8b314101-ab45-4dea-8336-3969a00f6204"
      },
      "source": [
        "Vidimo da je mreža savršeno naučila trening set. To se zove preprilagođavanje (overfitting). To je naravno nepoželjan rezultat, jer želimo da mreža može da radi dobro na podacima koji nisu deo trening seta.\n",
        "\n",
        "Možemo zaključiti da je veća vrednost tol zaustavila obučavanje pre nego što su se parametri mreže uskladili. Zapravo, izazvalo je rano zaustavljanje (early stopping), što je još jedna strategija za regularizaciju.\n",
        "\n",
        "Hajde da vidimo kakav će uticaj regularizacija imati na proces obučavanja."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3c7b0c5",
      "metadata": {
        "id": "d3c7b0c5",
        "outputId": "3882fdc0-4912-478b-eb55-b27f47430f85"
      },
      "outputs": [],
      "source": [
        "# %% tweak regularisation\n",
        "plt.figure()\n",
        "plt.scatter(xs, ys)\n",
        "plt.plot(x_axis, y_sin, ':', lw=4, alpha=0.2)\n",
        "plt.grid()\n",
        "for alpha in [1, 0.5, 0.1, 0.01]:\n",
        "    reg = neural_network.MLPRegressor(\n",
        "        hidden_layer_sizes=(5),\n",
        "        activation=\"tanh\",\n",
        "        alpha=alpha,\n",
        "        learning_rate_init=0.01,\n",
        "        max_iter=10000,\n",
        "        tol=1e-9,\n",
        "        random_state=42,\n",
        "        )\n",
        "    reg.fit(xs, ys)\n",
        "    y_preds = reg.predict(x_axis)\n",
        "    plt.plot(x_axis, y_preds, lw=2, alpha=0.5, label=alpha)\n",
        "\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a7dfec5-5902-45f1-bf6c-dff0f78dd496",
      "metadata": {
        "id": "9a7dfec5-5902-45f1-bf6c-dff0f78dd496"
      },
      "source": [
        "Vidimo efekat preterane regularizacije. U slučaju kada je $\\lambda$ 1, mreža uopšte ne uči iz trening seta. Za prevelike vrednosti (0,01) mreža se ponovo preprilagođava. Za vrednost 0,1 dobijamo najbolje rezultate ne samo u ovom eksperimentu, već i ukupno - za sve dosadašnje eksperimente."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
